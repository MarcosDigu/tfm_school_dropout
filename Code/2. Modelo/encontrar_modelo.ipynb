{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos datos depurados\n",
    "df = pd.read_csv('../../Data/dropout_depurado.csv', sep=',')\n",
    "\n",
    "# Separamos features de target\n",
    "y = df['Dropout']\n",
    "X = df.drop('Dropout', axis=1)\n",
    "\n",
    "# Separamos train de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un excel de resultados en el que almacenemos los resultados de cada modelo entrenado: la clase, la parametrizacion, la matriz de confusion en 4 columnas (TP, FP, FN, TN), y las variables input del modelo (si en el futuro el pretratamiento cambia, seremos capaces de rastrearlo y comparar distintos pretratamientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "\n",
    "# Crear un validador cruzado estratificado para conservar la proporción de clases en cada pliegue\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Importamos el excel creado para guardar los resultados\n",
    "operacion_resultados = pd.read_excel('tabla_resultados.xlsx')\n",
    "\n",
    "# Definimos la lista de modelos a probar\n",
    "models = [\n",
    "    DecisionTreeClassifier(),\n",
    "    LogisticRegression(max_iter=999999),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    XGBClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB(),  # Naive Bayes Classifier\n",
    "    AdaBoostClassifier(),  # AdaBoost Classifier\n",
    "    GradientBoostingClassifier(),  # Gradient Boosting Classifier\n",
    "    MLPClassifier()  # Multi-layer Perceptron Classifier\n",
    "]\n",
    "\n",
    "# Función para mostrar métricas y matriz de confusión\n",
    "def print_evaluation_metrics(model_name, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "                annot_kws={\"size\": 10}, linewidths=0.5, \n",
    "                xticklabels=['No Fraude', 'Fraude'], \n",
    "                yticklabels=['No Fraude', 'Fraude'])\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Verdadero')\n",
    "    plt.title(f'Matriz de Confusión - {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Entrenar y evaluar cada modelo con validación cruzada\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    # Crear el pipeline con MinMaxScaler y el modelo actual\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),  # Escalador MinMaxScaler\n",
    "        ('model', model)  # Modelo a entrenar\n",
    "    ])\n",
    "    \n",
    "    # Obtener predicciones de validación cruzada usando el pipeline\n",
    "    y_pred_cv = cross_val_predict(pipeline, X_train, y_train, cv=cv)\n",
    "    \n",
    "    # Obtener las métricas\n",
    "    cm = print_evaluation_metrics(f'{model_name} (Validación Cruzada)', y_train, y_pred_cv)\n",
    "    \n",
    "    # Capturando detalles del modelo\n",
    "    params = model.get_params() if hasattr(model, 'get_params') else 'default'\n",
    "    params_json = json.dumps(params)\n",
    "    input_variables = list(X_train.columns)\n",
    "    execution_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extraer TP, FP, FN, TN de la matriz de confusión\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Crear un nuevo DataFrame con los resultados\n",
    "    new_row = pd.DataFrame({\n",
    "        'Modelo': [model_name],\n",
    "        'Comentarios': \"Valores Latex Cuadro 1\",\n",
    "        'TP': [tp],\n",
    "        'FP': [fp],\n",
    "        'FN': [fn],\n",
    "        'TN': [tn],\n",
    "        'Parametrizacion': [params_json],\n",
    "        'Variables input del modelo': [input_variables],\n",
    "        'Fecha de ejecucion del modelo': [execution_date]\n",
    "    })\n",
    "    \n",
    "    # Concatenar el nuevo DataFrame con el DataFrame existente\n",
    "    operacion_resultados = pd.concat([operacion_resultados, new_row], ignore_index=True)\n",
    "\n",
    "    # Guardar el DataFrame de resultados actualizados\n",
    "    operacion_resultados.to_excel('tabla_resultados.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es para hacer la burrada de los 62k modelos de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "\n",
    "# Define the expanded parameter grid for XGBClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [None, 3],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'colsample_bytree': [0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.5, 1],\n",
    "    'reg_lambda': [1, 1.5],\n",
    "    'scale_pos_weight': [1, 2, 5],\n",
    "    'objective': ['binary:logistic', 'binary:hinge', 'binary:logitraw'],\n",
    "    'booster': ['gbtree', 'dart'],\n",
    "    'max_delta_step': [0, 1, 5],\n",
    "    'eval_metric': ['logloss', 'auc']\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "# Ruta del archivo\n",
    "file_path = 'tabla_resultados.xlsx'\n",
    "\n",
    "# Comprobar si el archivo existe\n",
    "if os.path.exists(file_path):\n",
    "    # Leer el archivo\n",
    "    operacion_resultados = pd.read_excel(file_path)\n",
    "else:\n",
    "    # Inicializar el DataFrame vacío\n",
    "    operacion_resultados = pd.DataFrame(columns=['Modelo', 'Parametrizacion', 'TP', 'FP', 'FN', 'TN', 'Variables input del modelo', 'Fecha de ejecucion del modelo', 'Comentarios'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import cupy as cp\n",
    "\n",
    "# Iniciar el tiempo al inicio del bucle\n",
    "start_time = time.time()\n",
    "\n",
    "# Pedir al usuario que especifique desde qué valor de i comenzar\n",
    "# start_i = int(input(\"Ingrese el valor inicial de i: \")) # Esto es en caso de que se desee detener el entrenamiento y reiniciar desde el mismo punto\n",
    "start_i = 0\n",
    "\n",
    "# Iterar sobre todas las combinaciones de parámetros\n",
    "for i, combination in enumerate(param_combinations[start_i:], start=start_i):\n",
    "    # Crear un diccionario de parámetros para el modelo\n",
    "    params = {key: combination[j] for j, key in enumerate(param_grid.keys())}\n",
    "\n",
    "    print(f'Probando con parametrizacion {i+1}/{len(param_combinations)} con parametros: {params}.')\n",
    "\n",
    "    # Crear y entrenar el modelo con esta configuración\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        gamma=params['gamma'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        objective=params['objective'],\n",
    "        booster=params['booster'],\n",
    "        max_delta_step=params['max_delta_step'],\n",
    "        tree_method='hist',\n",
    "        device='cuda',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=params['eval_metric'], \n",
    "        predictor='gpu_predictor'\n",
    "    )\n",
    "\n",
    "    model.fit(np.array(X_train), np.array(y_train))\n",
    "\n",
    "    # Predecir los valores de test\n",
    "    y_pred = model.predict(np.array(X_test))\n",
    "\n",
    "    # Calcular la matriz de confusión\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(conf_matrix)\n",
    "    print('\\n' + '-'*40 + '\\n')\n",
    "\n",
    "    # Añadir los valores de la nueva parametrización a la lista de resultados\n",
    "    new_row = {\n",
    "        'Modelo': 'XGBClassifier',\n",
    "        'Parametrizacion': params,\n",
    "        'TP': conf_matrix[1][1],\n",
    "        'FP': conf_matrix[0][1],\n",
    "        'FN': conf_matrix[1][0],\n",
    "        'TN': conf_matrix[0][0],\n",
    "        'Variables input del modelo': list(X_train.columns),\n",
    "        'Fecha de ejecucion del modelo': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'Comentarios': 'Entrenamiento real XGBoost'\n",
    "    }\n",
    "\n",
    "    # Concatenar el nuevo DataFrame con el DataFrame existente\n",
    "    operacion_resultados = pd.concat([operacion_resultados, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    # Guardar en el archivo Excel cada 100 iteraciones\n",
    "    if (i + 1) % 10 == 0:\n",
    "        operacion_resultados.to_excel(file_path, index=False)\n",
    "        print(f\"Guardado automático en el archivo Excel después de {i + 1} iteraciones.\")\n",
    "\n",
    "# Guardar el DataFrame actualizado en el archivo Excel al finalizar todas las iteraciones\n",
    "operacion_resultados.to_excel(file_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Grid search completed in {end_time - start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es para estudiar los 3 mejores modelos segun el codigo de antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ruta del archivo\n",
    "file_path = 'tabla_resultados.xlsx'\n",
    "\n",
    "# Cargar archivo si ya existe\n",
    "if os.path.exists(file_path):\n",
    "    operacion_resultados = pd.read_excel(file_path)\n",
    "else:\n",
    "    operacion_resultados = pd.DataFrame(columns=['Modelo', 'Parametrizacion', 'TP', 'FP', 'FN', 'TN', 'Variables input del modelo', 'Fecha de ejecucion del modelo', 'Comentarios'])\n",
    "\n",
    "# Diccionario de modelos y grids\n",
    "model_grid_dict = {\n",
    "    'GradientBoostingClassifier': (GradientBoostingClassifier(), param_grid_gb),\n",
    "    'SVC': (SVC(), param_grid_svc),\n",
    "    'LogisticRegression': (LogisticRegression(), param_grid_lr),\n",
    "}\n",
    "\n",
    "# Validación cruzada estratificada (3 folds)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Bucle que recorre cada modelo\n",
    "for model_name, (model, param_grid) in tqdm(model_grid_dict.items(), desc=\"Modelos\", unit=\"modelo\"):\n",
    "    # Obtener combinaciones de parámetros\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    # Bucle que recorre las combinaciones de parámetros con barra de progreso\n",
    "    for i, combination in enumerate(tqdm(param_combinations, desc=f\"{model_name} Grid Search\", unit=\"combinación\")):\n",
    "        # Crear diccionario de parámetros\n",
    "        params = {key: combination[j] for j, key in enumerate(param_grid.keys())}\n",
    "        \n",
    "        print(f'Ejecutando {model_name} con configuración {i+1}/{len(param_combinations)}: {params}')\n",
    "\n",
    "        # Crear el pipeline con MinMaxScaler y el modelo actual\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', MinMaxScaler()),  # Escalador MinMaxScaler para los campos numéricos\n",
    "            ('model', model.set_params(**params))  # Modelo con los parámetros configurados\n",
    "        ])\n",
    "\n",
    "        # Obtener predicciones con validación cruzada\n",
    "        y_pred_cv = cross_val_predict(pipeline, np.array(X_train), np.array(y_train), cv=cv)\n",
    "\n",
    "        # Calcular la matriz de confusión\n",
    "        conf_matrix = confusion_matrix(y_train, y_pred_cv)\n",
    "        # print(conf_matrix)\n",
    "\n",
    "        # Guardar resultados\n",
    "        new_row = {\n",
    "            'Modelo': model_name,\n",
    "            'Parametrizacion': params,\n",
    "            'TP': conf_matrix[1][1],\n",
    "            'FP': conf_matrix[0][1],\n",
    "            'FN': conf_matrix[1][0],\n",
    "            'TN': conf_matrix[0][0],\n",
    "            'Variables input del modelo': list(X_train.columns),\n",
    "            'Fecha de ejecucion del modelo': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'Comentarios': f'Entrenamiento {model_name} con validación cruzada de 3 folds'\n",
    "        }\n",
    "\n",
    "        operacion_resultados = pd.concat([operacion_resultados, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        # Guardar en Excel cada 10 iteraciones\n",
    "        if (i + 1) % 10 == 0:\n",
    "            operacion_resultados.to_excel(file_path, index=False)\n",
    "            print(f'Guardado automático después de {i + 1} iteraciones.')\n",
    "\n",
    "# Guardar los resultados finales\n",
    "operacion_resultados.to_excel(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es para generar la tabla de interpretabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes del modelo de regresión logística:\n",
      "                  Feature  Coefficient  Odds Ratio\n",
      "0                     Age     0.028480    1.028889\n",
      "1       Unemployment_rate     0.113560    1.120259\n",
      "2          Inflation_rate    -0.036043    0.964599\n",
      "3                     GDP     0.027682    1.028068\n",
      "4               Avg_grade    -0.016160    0.983970\n",
      "..                    ...          ...         ...\n",
      "237              Gender_1     0.218717    1.244479\n",
      "238  Scholarship_holder_0     0.240485    1.271866\n",
      "239  Scholarship_holder_1    -0.239497    0.787024\n",
      "240       International_0     0.127612    1.136112\n",
      "241       International_1    -0.126624    0.881065\n",
      "\n",
      "[242 rows x 3 columns]\n",
      "\n",
      "Intercepto del modelo:\n",
      "1.2589218173192143\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[2135  117]\n",
      " [ 270  796]]\n",
      "\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      2252\n",
      "           1       0.87      0.75      0.80      1066\n",
      "\n",
      "    accuracy                           0.88      3318\n",
      "   macro avg       0.88      0.85      0.86      3318\n",
      "weighted avg       0.88      0.88      0.88      3318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Parámetros del mejor modelo de logistic regresion chavales\n",
    "params = {'C': 1, 'penalty': 'l2', 'class_weight': None, 'solver': 'lbfgs', 'max_iter': 999999}\n",
    "\n",
    "# Entrenar el modelo con los parámetros dados\n",
    "log_reg_model = LogisticRegression(**params)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los coeficientes\n",
    "coef = log_reg_model.coef_[0]\n",
    "intercept = log_reg_model.intercept_[0]\n",
    "\n",
    "# Crear un DataFrame para los coeficientes\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coef\n",
    "})\n",
    "\n",
    "# Calcular los odds ratios\n",
    "coef_df['Odds Ratio'] = np.exp(coef_df['Coefficient'])\n",
    "\n",
    "# Suponiendo que 'Feature_1' es una variable categórica para la cual calcularemos WOE\n",
    "# Nota: Debes reemplazar 'Feature_1' con el nombre de la característica real en tu DataFrame\n",
    "if 'Feature_1' in X_train.columns:\n",
    "    # Calcular la proporción de eventos y no eventos\n",
    "    event_rate = y_train.mean()  # Proporción de eventos\n",
    "    non_event_rate = 1 - event_rate  # Proporción de no eventos\n",
    "\n",
    "    # Calcular WOE para cada categoría en 'Feature_1'\n",
    "    feature_1_df = pd.concat([X_train[['Feature_1']], y_train], axis=1)\n",
    "    woe_df = feature_1_df.groupby('Feature_1').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Proporción de Eventos': x[y_train == 1].shape[0] / y_train.shape[0],\n",
    "            'Proporción de No Eventos': x[y_train == 0].shape[0] / y_train.shape[0]\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    woe_df['WOE'] = np.log(woe_df['Proporción de Eventos'] / woe_df['Proporción de No Eventos'])\n",
    "    coef_df = coef_df.merge(woe_df[['Feature_1', 'WOE']], left_on='Feature', right_on='Feature_1', how='left')\n",
    "\n",
    "    # Limpiar el DataFrame resultante\n",
    "    coef_df = coef_df.drop(columns=['Feature_1'])\n",
    "\n",
    "# Imprimir los coeficientes y los odds ratios\n",
    "print(\"Coeficientes del modelo de regresión logística:\")\n",
    "print(coef_df)\n",
    "\n",
    "print(\"\\nIntercepto del modelo:\")\n",
    "print(intercept)\n",
    "\n",
    "# Evaluar el rendimiento del modelo en el conjunto de entrenamiento\n",
    "# Predicciones\n",
    "y_pred = log_reg_model.predict(X_train)\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Imprimir el informe de clasificación\n",
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.to_excel('coeficientes_mejor_log.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
